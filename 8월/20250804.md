### :link: 2025-08-04
- [Retrieval 기반 LangChain](#retrieval-기반-langchain)
 
&nbsp;
### Retrieval 기반 LangChain

LLM 단독 응답이 아닌, **외부 문서 기반 질문 응답 시스템(RAG)** 을 만들기 위한 실습.

핵심 구성:  
문서 수집 → 벡터화 → 저장소 구성 → 유사도 검색 → LLM 응답

**라이브러리 설치 및 환경 설정**

```python
%pip install langchain langchain-community langchain-openai pypdf faiss-cpu sentence-transformers beautifulsoup4
````

```python
from dotenv import load_dotenv
load_dotenv()
```

### 문서 로딩

#### 1) 웹 문서 불러오기: `WebBaseLoader`

```python
from langchain_community.document_loaders import WebBaseLoader

url = "https://ko.wikipedia.org/wiki/기니피그"
loader = WebBaseLoader(url)
documents = loader.load()
```

* 결과:

  * `documents[0].page_content` → 문서 본문
  * `documents[0].metadata` → 제목, 언어, URL 등 메타데이터 포함

#### 2) PDF 문서 불러오기: `PyPDFLoader`

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("The_Adventures_of_Tom_Sawyer.pdf")
documents = loader.load()
```

* 결과:

  * 35페이지 분량 로딩
  * 각 `Document` 객체에 `page_content` 및 `metadata` 포함

### 임베딩 처리

#### 1) 단일 문장 임베딩

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vector = embeddings.embed_query("The quick brown fox jumps over the lazy dog")
print(len(vector))  # → 1536차원
```

#### 2) 문서 임베딩 (전체 PDF 등)

```python
docs = [doc.page_content for doc in documents]
vects = embeddings.embed_documents(docs)
```

* 출력:

  * `print(len(vects), len(vects[0]))` → (35, 1536)
  * `pandas.DataFrame(vects)`로 시각화 가능

### 벡터스토어 구성

#### 1) FAISS를 이용한 벡터 저장소 구성

```python
from langchain.vectorstores import FAISS

vector_store = FAISS.from_documents(documents, embeddings)
```

#### 2) 유사도 검색 (k-NN 기반)

```python
vector_store.similarity_search("Tom Sawyer", k=3)
```

* 관련 문서 상위 3개 반환

### Retriever 생성 및 활용

#### 1) Retriever 객체 변환

```python
retriever = vector_store.as_retriever()
```

* `.as_retriever()` → VectorStore → Retriever로 변환

#### 2) RetrievalQA 체인 구성 및 실행

```python
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

model = ChatOpenAI(model="gpt-4o", temperature=0)

retriever_qa = RetrievalQA.from_chain_type(
    llm=model,
    retriever=retriever,
    chain_type='stuff'  # 문서들을 모두 하나로 묶어서 LLM에 전달
)

response = retriever_qa.invoke("톰소여는 어떤 사람인가요?")
print(response)
```

* 출력 예시:

```json
{
  "query": "톰소여는 어떤 사람인가요?",
  "result": "톰 소여는 모험을 사랑하는 소년입니다... 호기심 많고 용감하며 때로는 장난꾸러기 같은 면모도 가지고 있습니다."
}
```

| 구성 요소              | 설명                   |
| ------------------ | -------------------- |
| `WebBaseLoader`    | 웹 문서 로딩용             |
| `PyPDFLoader`      | PDF 문서 로딩용           |
| `OpenAIEmbeddings` | 텍스트를 벡터로 변환          |
| `FAISS`            | 벡터를 저장하고 유사도 검색      |
| `RetrieverQA`      | 검색 + GPT 응답을 연결하는 체인 |