### :link: 2025-08-01
- [FFMPEG 설치 오류와 MacPort 설치](#0-ffmpeg-설치-오류와-macport-설치)
- [Voice Chat](#1-voice-chat)
 
&nbsp;
### 0. FFMPEG 설치 오류와 MacPort 설치
이 실습은 FFMPEG 라 불리는 미디어 파일 변환 프로그램을 설치했어야 했다.
나는 맥을 사용하고 있어서, homebrew로 설치를 시도했었다.
```zsh
$ brew install ffmpeg
```
그런데, 설치가 자꾸 중간에 끊기면서 해시 불일치 오류가 발생했다.
```zsh
Error: mbedtls: SHA256 mismatch
```
여러 방법을 사용했지만, 결국 문제는 Homebrew가 나의 맥 OS를 완전하게 지원하지 않았기 때문이었다.   
사실 설치할 때부터 버전이 너무 낮다는 경고창이 있었는데 그래도 설치가 우선이었기 때문에 무시하고 설치했었다.   
결국엔 설치도, 몇몇 brew 명령어도 잘 실행되었기 때문에 비공식 지원이라도 안정적으로 동작하는구나.. 하고 넘겼었다.  

그런데, 이번엔 정말 무슨 방법을 해도 안됐다.

---

결국 Homebrew는 삭제하고 나의 macOS Monterey (12.7.6) 을 완전히 지원하는 패키지 매니저, MacPort를 사용하여 해결했다.
 
MacPort는 [공식 홈페이지 > Installing MacPorts 메뉴](https://www.macports.org/install.php)에서 본인에게 맞는 OS 버전을 선택하여 .pkg 파일을 다운로드하고 설치하면 된다.

MacPort에서 FFMPEG를 설치하는 명령어는 다음과 같다
```zsh
$ sudo port install ffmpeg
```
MacPort로 설치하는 도중에는 해시 관련 오류도 나지 않고 아주 잘 설치가 되었다.

또한, 아래 명령어로 설치 경로를 확인할 수 있다.

```zsh
# 입력한 명령어
$ which ffprobe

# 출력
/opt/local/bin/ffprobe
```
    
**결론**: 본인의 개발 환경을 지원하는 서비스를 사용하자 (당연한 소리 ^..^)
 
&emsp; 
&nbsp;
 
### 1. Voice Chat
다음의 순서로 동작하는 프로그램의 실습을 진행했다.   
   
    1) 화면상에서 사용자가 질문을 녹음한다.   
    2) ai가 해당 오디오를 텍스트로 변환한다.   
    3) 위 텍스트를 질문으로 하여 AI를 호출한다.   
    4) 질문과 답변을 화면에 채팅 형식으로 호출한다.   
```py
import streamlit as st
from audiorecorder import audiorecorder
from streamlit_chat import message as msg
import openai_api

def main():
    st.set_page_config(
        page_title='Voice Chatbot',
        page_icon="👀",
        layout='wide'
    )
    st.header('Voice Chatbot')
    st.markdown('---')

    with st.expander('Voice Chatbot 프로그램을 사용하는 방법', expanded=False):
        st.write(
            """ 
            1. 녹음하기 버튼을 눌러 질문을 녹음합니다.
            2. 녹음이 완료되면 자동으로 Whisper 모델을 이용해 음성을 텍스트로 변환 후 LLM에 질의합니다.
            3. LLM이 응답을 다시 TTS 모델을 사용해 음성으로 변호나하고 이를 사용자에게 응답합니다.
            4. LLM은 OpenAI 사의 GPT 모델을 사용합니다.
            5. 모든 질문/답변은 텍스트로도 제공합니다.
            """
        )
    
    system_instruction = '당신은 친절한 챗봇입니다.'

    # session state 초기화
    # - chats: 웹 페이지 시각화용 대화내역
    # - messages: LLM 질의/웹페이지 시각화를 위한 대화내역
    # - check_reset: 사이드바 초기화 버튼 활성화용

    if 'messages' not in st.session_state:
        # 대화 내역을 state로 담아서 ai가 대화 내용을 기억하게 함
        # 앱이 처음 실행될 때 딱 한번만 초기화되도록 함
        st.session_state['messages'] = [
            {'role': 'system', 'content': system_instruction}
        ]
    
    if 'check_reset' not in st.session_state:
        st.session_state['check_reset'] = False

    with st.sidebar:
            model = st.radio(label='GPT 모델', options=['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o'], index=2)
            print(model)

            if st.button(label='초기화'):
                st.session_state['messages'] = [
                    {'role': 'system', 'content': system_instruction}
                ]
                st.session_state['check_reset'] = True

    col1, col2 = st.columns(2) # 화면을 두개의 열로 나누고, 각 화면을 변수에 담아줍니다
    with col1:
        st.subheader('녹음하기')
        
        audio = audiorecorder()

        if (audio.duration_seconds > 0) and (st.session_state['check_reset'] == False):
            # 화면상의 재생 기능
            st.audio(audio.export().read())
            query = openai_api.stt(audio)
            print('Q : ', query)
            # 변환된 텍스트를 사용자의 메시지로 session_state에 추가해서 대화 기록을 남긴다.
            st.session_state['messages'].append({'role':'user', 'content': query})
            response = openai_api.ask_gpt(st.session_state['messages'], model) # 사이드바에서 선택한 모델 전달 
            print('A : ', response)

            # GPT 답변도 session_state에 추가해준다.
            st.session_state['messages'].append({"role":"assistant", "content": response})

            #GPT 답변을 음성으로 변환
            audio_tag = openai_api.tts(response)
            st.html(audio_tag) # 시각화되지 않고, 자동으로 재생 (즉시 재생 가능한 html 태그)

    with col2:
        st.subheader('질문/답변')
        if(audio.duration_seconds > 0) and (st.session_state['check_reset'] == False ):
            for i, message in enumerate(st.session_state['messages']): # index값까지 활용 목적
                role = message['role']
                content = message['content']
                if role == 'user':
                    msg(content, is_user=True, key=str(i), avatar_style="big-smile")
                elif role == 'assistant':
                    msg(content, is_user=False, key=str(i), avatar_style="fun-emoji")
        else:
            st.session_state['check_reset'] = False # 만약 대화가 없다면 초기화


if __name__ == '__main__':
    main()
```

```py
from dotenv import load_dotenv
from openai import OpenAI
import base64
import os

load_dotenv()

client = OpenAI()

def stt(audio):
    filename = 'temp.mp3' # 오디오 데이터를 저장할 임시 파일 이름
    audio.export(filename, format='mp3')

    with open(filename, 'rb') as f:
        transcription = client.audio.transcriptions.create(
            model="whisper-1",
            file=f
        )
    os.remove(filename) # 임시파일 삭제
    return transcription.text

def ask_gpt(messages, model):
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=1,
        max_tokens=4096,
        top_p=1
    )
    return response.choices[0].message.content

def tts(text):
    filename = "output.mp3"
    with client.audio.speech.with_streaming_response.create(
        model='tts-1',
        voice='echo',
        input=text
    ) as response:
        response.stream_to_file(filename)

    #Base64 인코딩 : 음성 파일 데이터 자체를 아주 긴 텍스트 (base64)로 변환한다.
    with open(filename, 'rb') as f:
        data = f.read()
        b64_encoded = base64.b64encode(data).decode()
        audio_tag = f""" 
        <audio autoplay="true">
            <source src="data:audio/mp3;base64,{b64_encoded}" type='audio/mp3'/>
        </audio>
        """
    os.remove(filename) # 원본 삭제

    return audio_tag # 파일이 아닌, 오디오 적보가 담긴 HTML 코드 조각을 반환
```
